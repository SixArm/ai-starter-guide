Common metrics include accuracy, precision, recall, F1-score, mean squared error (MSE), and mean absolute error (MAE).

Attention: In the context of neural networks, attention mechanisms help the model focus on relevant parts of the input when producing an output.

Bias: Assumptions made by an AI model about the data. A “bias variance tradeoff” is the balance that must be achieved between assumptions a model makes about the data and the amount a model’s predictions change, given different training data. Inductive bias is the set of assumptions that a machine learning algorithm makes on the underlying distribution of the data.

ChatGPT: A large-scale AI language model developed by OpenAI that generates human-like text.

Compute: The computational resources (like CPU or GPU time) used in training or running AI models.

Data Augmentation: The process of increasing the amount and diversity of data used for training a model by adding slightly modified copies of existing data.

Diffusion: In AI and machine learning, a technique used for generating new data by starting with a piece of real data and adding random noise. A diffusion model is a type of generative model in which a neural network is trained to predict the reverse process when random noise is added to data. Diffusion models are used to generate new samples of data that are similar to the training data.

Embedding: The representation of data in a new form, often a vector space. Similar data points have more similar embeddings.

End-to-End Learning: A type of machine learning model that does not require hand-engineered features. The model is simply fed raw data and expected to learn from these inputs.

Fine-tuning: The process of taking a pre-trained machine learning model that has already been trained on a large dataset and adapting it for a slightly different task or specific domain. During fine-tuning, the model’s parameters are further adjusted using a smaller, task-specific dataset, allowing it to learn task-specific patterns and improve performance on the new task.

Foundation Model: Large AI models trained on broad data, meant to be adapted for specific tasks.

Generative AI: A branch of AI focused on creating models that can generate new and original content, such as images, music, or text, based on patterns and examples from existing data.
  
Hidden Layer: Layers of artificial neurons in a neural network that are not directly connected to the input or output.

Inference: The process of making predictions with a trained machine learning model.

Instruction Tuning: A technique in machine learning where models are fine-tuned based on specific instructions given in the dataset.

Latent Space: In machine learning, this term refers to the compressed representation of data that a model (like a neural network) creates. Similar data points are closer in latent space.

Mixture of Experts: A machine learning technique where several specialized submodels (the “experts”) are trained, and their predictions are combined in a way that depends on the input.

Multimodal: In AI, this refers to models that can understand and generate information across several types of data, such as text and images.
 
Objective Function: A function that a machine learning model seeks to maximize or minimize during training.

Parameters: In machine learning, parameters are the internal variables that the model uses to make predictions. They are learned from the training data during the training process. For example, in a neural network, the weights and biases are parameters.

Pre-training: The initial phase of training a machine learning model where the model learns general features, patterns, and representations from the data without specific knowledge of the task it will later be applied to. This unsupervised or semi-supervised learning process enables the model to develop a foundational understanding of the underlying data distribution and extract meaningful features that can be leveraged for subsequent fine-tuning on specific tasks.

Prompt: The initial context or instruction that sets the task or query for the model.

Regularization: In machine learning, regularization is a technique used to prevent overfitting by adding a penalty term to the model’s loss function. This penalty discourages the model from excessively relying on complex patterns in the training data, promoting more generalizable and less prone-to-overfitting models.

RLHF (Reinforcement Learning from Human Feedback): A method to train an AI model by learning from feedback given by humans on model outputs.
 
Singularity: In the context of AI, the singularity (also known as the technological singularity) refers to a hypothetical future point in time when technological growth becomes uncontrollable and irreversible, leading to unforeseeable changes to human civilization.

TensorFlow: An open-source machine learning platform developed by Google that is  used to build and train machine learning models.

Transfer Learning: A method in machine learning where a pre-trained model is used on a new problem.

LSTM and GRU variants of RNNs

binary classification
