# True positive rate (TPR)

The term "true positive rate" (TPR) is a common metric used in the evaluation of machine learning models, including those used in artificial intelligence (AI). It is also known as sensitivity, recall, or hit rate. The true positive rate measures the proportion of actual positive instances correctly identified by the model out of the total actual positive instances.

The formula for calculating the true positive rate is:

$$TPR=\frac{True Positives}(True Positives+False Negatives}$$

Where:

* True Positives (TP): The number of instances that are actually positive and are correctly identified as positive by the model.

* False Negatives (FN): The number of instances that are actually positive but are incorrectly identified as negative by the model.

The true positive rate provides insights into how well a model performs in correctly identifying positive instances, which is especially important in scenarios where the cost of missing positive instances (false negatives) is high. For example, in medical diagnostics, correctly identifying individuals with a particular condition is crucial, and false negatives (missing actual cases) can have serious consequences.

In the context of AI and machine learning, the true positive rate is often used alongside other evaluation metrics such as precision, specificity, accuracy, and the F1 score to provide a comprehensive assessment of a model's performance.
